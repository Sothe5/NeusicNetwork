{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import librosa as lb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize random biases\n",
    "def create_biases(number_of_neurons, n_classes):\n",
    "    biases = []\n",
    "    #Hidden layers\n",
    "    for i in range(len(number_of_neurons)):\n",
    "        biases.append(tf.Variable(tf.random_normal([number_of_neurons[i]])))\n",
    "    #Output Layer\n",
    "    biases.append(tf.Variable(tf.random_normal([n_classes])))\n",
    "    return biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize random weights\n",
    "def create_weights(number_of_neurons, n_inputs, n_classes):\n",
    "    weights = []\n",
    "    #First layer (nºinputs x nºneurons)\n",
    "    weights.append(tf.Variable(tf.random_normal([n_inputs,number_of_neurons[0]])))\n",
    "    \n",
    "    #Hidden layers\n",
    "    for i in range(1,len(number_of_neurons)):\n",
    "        weights.append(tf.Variable(tf.random_normal([number_of_neurons[i-1],number_of_neurons[i]])))\n",
    "    #Output layer (nºneuros x nºoutputs)\n",
    "    weights.append(tf.Variable(tf.random_normal([number_of_neurons[len(number_of_neurons)-1],\n",
    "                                                                                  n_classes])))    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    progression = []\n",
    "    activation = []\n",
    "    progression.append(tf.add(tf.matmul(x, weights[0]), biases[0]))\n",
    "    activation.append(tf.nn.tanh(progression[0]))\n",
    "    for i in range(1, len(number_of_neurons)+1):\n",
    "        progression.append(tf.add(tf.matmul(activation[i-1], weights[i]), biases[i]))\n",
    "        activation.append(tf.nn.tanh(progression[i]))\n",
    "    \n",
    "    return activation[len(number_of_neurons)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getDataFrom(path):\n",
    "    data = []\n",
    "    with open(path, 'rb') as f:\n",
    "        content = f.read()\n",
    "        data = pickle.loads(content)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = getDataFrom('/run/media/zolastro/Storage/DatasetNeusic/inputs')\n",
    "data = np.reshape(data, (29*1000, 13*5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = getDataFrom('/run/media/zolastro/Storage/DatasetNeusic/labels')\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Input and correct output\n",
    "x = tf.constant(data, dtype='float')\n",
    "y = tf.constant(labels, dtype='float')\n",
    "\n",
    "learning_rate = 0.003\n",
    "training_epochs = 600\n",
    "n_samples = 29*1000 # Number of samples we have\n",
    "n_inputs = 13*5    #We get a matrix (13 x 5) when calculating the MFCC\n",
    "n_classes = 10     #Rock, blues, jazz...\n",
    "batch_size = int(n_samples/1) #Make a 10-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Create our Neural Network\n",
    "number_of_neurons = [256, 256, 256, 256]\n",
    "biases = create_biases(number_of_neurons, n_classes)\n",
    "weights = create_weights(number_of_neurons, n_inputs, n_classes)\n",
    "#Get predictions\n",
    "pred = multilayer_perceptron(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set cost and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inizialize variables for TensorFlow\n",
    "init = tf.global_variables_initializer()\n",
    "#Start session\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "x.eval(session=sess)\n",
    "y.eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRandomData():\n",
    "    in_x = np.random.randn(n_samples, n_inputs)\n",
    "    in_y = np.zeros((n_samples, n_classes))\n",
    "    for i in range(n_samples):\n",
    "        in_y[i][randint(0, n_classes-1)] = 1\n",
    "    return in_x, in_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in_x, in_y = getRandomData()\n",
    "in_x, in_y = x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def taking_batches(input_x, expected_y, batch_size, offset):\n",
    "    return input_x[offset:offset+batch_size],expected_y[offset:offset+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.0\n",
    "    total_batches = int(n_samples/batch_size)\n",
    "    for i in range(total_batches): \n",
    "        batch_x,batch_y = taking_batches(in_x,in_y,batch_size,i)\n",
    "        \n",
    "        \n",
    "        _,c = sess.run([optimizer,cost], feed_dict={x:batch_x.eval(session=sess),y:batch_y.eval(session=sess)}) \n",
    "        avg_cost += c/total_batches;\n",
    "    clear_output()\n",
    "    #print(\"Epoch: {} cost= {:.4f}\".format(epoch+1,avg_cost))\n",
    "    \n",
    "print(\"Finished with {} epochs with cost {}\".format(training_epochs,avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f6b6b1b83d07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorrect_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcorrect_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Test error\n",
    "correct_predictions = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "correct_predictions = tf.cast(correct_predictions, \"float\")\n",
    "accuracy = tf.reduce_mean(correct_predictions)\n",
    "print(\"Accuracy: {}\".format(accuracy.eval({x:batch_x.eval(session=sess),y:batch_y.eval(session=sess)})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
